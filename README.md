# AlpaCare LoRA Fine-Tuning on LLaMA-1B

This repository provides code, notebooks, and scripts for fine-tuning a **LLaMA-1B** model on the **AlpaCare-MedInstruct-52k** medical instruction dataset using **LoRA (Low-Rank Adaptation)**. The project includes preprocessing, training, evaluation, and inference pipelines.

---

## âš¡ Project Overview

- **Base Model:** LLaMA-1B Instruct  
- **Adapter Method:** LoRA (Low-Rank Adaptation)  
- **Dataset:** [lavita/AlpaCare-MedInstruct-52k](https://huggingface.co/lavita/AlpaCare-MedInstruct-52k)  
- **Training Type:** FP16 fine-tuning  
- **Evaluation:** Human evaluation with rubric and safety scoring  

**Disclaimer:** This model is for research purposes only. Outputs **are not guaranteed to be clinically accurate**. Do not use this model for real medical advice or patient care.

---

## ðŸ—‚ Repository Structure

